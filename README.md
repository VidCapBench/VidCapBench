# VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation

## Overview
VidCapBench is the first evaluation benchmark designed for assessing video captions in controllable text-to-video (T2V) generation. 

To cater for T2V, we have made the following endeavers in data curation:

* ***Diverse videos.*** Besides collecting videos from open-source datasets, we also use videos from YouTube and user-generated content (UGC) platforms to ensure a portion of our data remains unexposed to prior training or processing. When collecting videos, we pay much attention to the diversity, resulting in various subject categories, art styles, subjects amounts, video length, and so on.

<p align="center">
    <img src="./assets/video_distribution.png" width="94%" height="100%">
</p>

* ***Diverse QA pairs that aligned with T2V evaluation.*** VidCapBench evaluate captioning models in four dimensions: Video Aesthetics, Video Content, Video Motion, and Physical Laws, which align with the key metrics of T2V generation. The QA pairs are first generated by GPT-4o and expert classifiers, then we mannually revise those unsuitable ones and annotate additional QA pairs to address dimension imbalances.

    Through analization, we find that in the predefined-QA evaluation paradigm, there exists a subset of QA pairs that cannot be evaluated through automated methods because they show a great deal of inconsistency in their repetitive machine evaluations. Thus, we split the QA pairs in VidCapBench into two subsets: _VidCapBench-AE_ <img src="./assets/AE.png" width="20" height="20">, which receives consistent machine evaluations so that can be evaluated automatedly; and _VidCapBench-HE_ <img src="./assets/HE.png" width="18" height="18">, the more challenging QA pairs that necessitate human intervention for nuanced differentiation.

<p align="center">
    <img src="./assets/demo_qa_distribution.png" width="94%" height="100%">
</p>

* ***Training-free T2V verification.*** We have made training-free T2V verification to validate that there is a significant positive correlation between scores on VidCapBench and T2V quality metrics, demonstrating that VidCapBench can provide valuable guidance for training T2V models. We provide a qualitive illustration below. In this case, the video is associated with nine QA pairs in VidCapBench-AE and four QA pairs in VidCapBench-HE. The similarity between the generated video and the original video, as well as the overall generation quality, are strongly correlated with the evaluation results in VidCapBench. Among the captioning models compared, Gemini exhibits the best performance. 
<p align="center">
    <img src="./assets/casestudy.png" width="94%" height="100%">
</p>

## Evaluation
First, you have to utilize your captioning models to generate captions for the videos, and save the captions in JSONL format, following this structure:
```
{"question": "Describe the video in detail.", "video": "Tarsier_20.mp4", "model_generation": Caption}
```
